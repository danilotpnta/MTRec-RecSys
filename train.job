#!/bin/bash

#SBATCH --partition=gpu
# Apparently we are now charged for 2 GPUs automatically, so might as well. Change this to 1 if model training errors arise.
#SBATCH --gpus=1
#SBATCH --gpus-per-node=1
#SBATCH --job-name=RECSYS
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=12
#SBATCH --time=23:59:00
#SBATCH --mem=120G
#SBATCH --output=/home/scur1595/MTRec-RecSys/%A.out

date


WORK_DIR=$HOME/MTRec-RecSys
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $WORK_DIR/.venv/bin/activate

export HF_DATASETS_CACHE=/scratch-local/$(whoami)
export HF_DATASETS_CACHE=$HOME/data
rm -rf $HF_DATASETS_CACHE


export HF_DATASETS_IN_MEMORY_MAX_SIZE=118719476736 # 120GB
rm -rf $HF_DATASETS_CACHE/large
python -m src.recsys.train \
    --data $HF_DATASETS_CACHE \
    --dataset large --epochs 5 --lr 2e-4 --num_workers 0
