{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTRec:\n",
    "\n",
    "Given $I$ number historical clicked news of a user $ N^h = [n_1^h , n_2^h, ..., n^h_I ]$ and a set of $J$ candidate news $ N^c = [n^c_1, n^c_2, ..., n^c_J ] $, our goal is to calculate the user interest score $s_j$ of each candidate news according to the historical behavior of the user, then the candidate news with the highest interest score is recommended to the user. \n",
    "\n",
    "For each news, we have its title text T , category label $p^c$, and entity set E. \n",
    "\n",
    "## 2.1 News Recommendation Framework\n",
    "\n",
    "As shown in Figure 2, there are three main components in news recommendation framework, i.e., a news encoder, a user encoder, and a click predictor. \n",
    "### News Encoder\n",
    "For each news n, we encode its title with pre-trained BRET (Devlin et al., 2019). Specifically, we feed the tokenized text T into the BERT model and **adopt the embedding of [CLS] token as the news representation r**. \n",
    "\n",
    "We denote the encoded vectors of historical clicked news $N^h$ and candidate news $N^c$ as $R^h = [r^h_1 , r^h_2 , ..., r^h_I ]$ and $R^c = [r^c_1, r^c_2, ..., r^c_J ]$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 22:09:39.427646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation & Maths imports\n",
    "import os.path\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from pprint import pprint\n",
    "\n",
    "# Torch & Transformer imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer, \n",
    "    RobertaModel, \n",
    "    RobertaTokenizer, \n",
    "    XLMRobertaModel, \n",
    "    XLMRobertaTokenizer\n",
    ")\n",
    "\n",
    "# ebrec constants\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,         # article_id\n",
    "    DEFAULT_TITLE_COL,              # title\n",
    "    DEFAULT_BODY_COL,               # body\n",
    "    DEFAULT_SUBTITLE_COL,           # subtitle\n",
    "    DEFAULT_TOPICS_COL,             # topics\n",
    "    DEFAULT_CATEGORY_STR_COL,       # category_str\n",
    "    DEFAULT_LABELS_COL,             # labels\n",
    "    DEFAULT_USER_COL,               # user_id\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL, # article_id_fixed\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,    # article_ids_inview\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,   # article_ids_clicked\n",
    "    DEFAULT_IMPRESSION_ID_COL       # impression_id\n",
    ")\n",
    "\n",
    "# ebrec utils\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._behaviors import truncate_history, create_binary_labels_column\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "from ebrec.utils._python import (\n",
    "    generate_unique_name,\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    "    create_lookup_dict,\n",
    ")\n",
    "\n",
    "# Columns to be used in the dataset processing\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,                # \"user_id\"\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,  # \"article_id_fixed\"\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,     # \"article_ids_inview\"\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,    # \"article_ids_clicked\"\n",
    "    DEFAULT_IMPRESSION_ID_COL,       # \"impression_id\"\n",
    "]\n",
    "DEFAULT_TOKENS_COL = \"tokens\"\n",
    "N_SAMPLES_COL = \"n_samples\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datoapanta/Desktop/1. Coding/MTRec-RecSys/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embeddings Shape: (125541, 2)\n",
      "Sample Embedding: shape: (5, 2)\n",
      "┌────────────┬───────────────────────────────────┐\n",
      "│ article_id ┆ FacebookAI/xlm-roberta-base       │\n",
      "│ ---        ┆ ---                               │\n",
      "│ i32        ┆ list[f32]                         │\n",
      "╞════════════╪═══════════════════════════════════╡\n",
      "│ 3000022    ┆ [0.102449, 0.101148, … -0.020715… │\n",
      "│ 3000063    ┆ [0.107297, 0.103073, … 0.004873]  │\n",
      "│ 3000613    ┆ [0.125139, 0.124621, … -0.05177]  │\n",
      "│ 3000700    ┆ [0.105697, 0.076335, … -0.034872… │\n",
      "│ 3000840    ┆ [0.098175, 0.114629, … -0.024436… │\n",
      "└────────────┴───────────────────────────────────┘\n",
      "Joined Articles with Embeddings: (11777, 7)\n",
      "Sample Article Embedding: shape: (5, 7)\n",
      "┌────────────┬──────────────┬──────────────┬─────────────┬─────────────┬─────────────┬─────────────┐\n",
      "│ article_id ┆ title        ┆ body         ┆ subtitle    ┆ topics      ┆ category_st ┆ tokens      │\n",
      "│ ---        ┆ ---          ┆ ---          ┆ ---         ┆ ---         ┆ r           ┆ ---         │\n",
      "│ i32        ┆ str          ┆ str          ┆ str         ┆ list[str]   ┆ ---         ┆ list[f32]   │\n",
      "│            ┆              ┆              ┆             ┆             ┆ str         ┆             │\n",
      "╞════════════╪══════════════╪══════════════╪═════════════╪═════════════╪═════════════╪═════════════╡\n",
      "│ 3037230    ┆ Ishockey-spi ┆ Ambitionerne ┆ ISHOCKEY:   ┆ [\"Kriminali ┆ sport       ┆ [0.111132,  │\n",
      "│            ┆ ller: Jeg    ┆ om at komme  ┆ Ishockey-sp ┆ tet\",       ┆             ┆ 0.119653, … │\n",
      "│            ┆ troede jeg…  ┆ til USA…     ┆ illeren     ┆ \"Kendt\", …  ┆             ┆ -0.008356…  │\n",
      "│            ┆              ┆              ┆ Seb…        ┆ \"Min…       ┆             ┆             │\n",
      "│ 3044020    ┆ Prins Harry  ┆ Den britiske ┆ Hoffet      ┆ [\"Kriminali ┆ underholdni ┆ [0.113472,  │\n",
      "│            ┆ tvunget til  ┆ tabloidavis  ┆ tvang Prins ┆ tet\",       ┆ ng          ┆ 0.097556, … │\n",
      "│            ┆ dna-test     ┆ The Sun…     ┆ Harry til   ┆ \"Kendt\", …  ┆             ┆ -0.016068…  │\n",
      "│            ┆              ┆              ┆ at …        ┆ \"Per…       ┆             ┆             │\n",
      "│ 3057622    ┆ Rådden       ┆ Slingrende   ┆ Kan ikke    ┆ [\"Kriminali ┆ nyheder     ┆ [0.090726,  │\n",
      "│            ┆ kørsel på    ┆ spritkørsel. ┆ straffes:   ┆ tet\", \"Tran ┆             ┆ 0.125526, … │\n",
      "│            ┆ blå plader   ┆ Grove ov…    ┆ Udenlandske ┆ sportmidde… ┆             ┆ -0.011349…  │\n",
      "│            ┆              ┆              ┆ d…          ┆             ┆             ┆             │\n",
      "│ 3073151    ┆ Mærsk-arving ┆ To oldebørn  ┆ FANGET I    ┆ [\"Erhverv\", ┆ nyheder     ┆ [0.129573,  │\n",
      "│            ┆ er i         ┆ af           ┆ FLODBØLGEN: ┆ \"Privat vir ┆             ┆ 0.132865, … │\n",
      "│            ┆ livsfare     ┆ skibsreder   ┆ Skibsredere ┆ ksomhed\",…  ┆             ┆ -0.009751…  │\n",
      "│            ┆              ┆ Mærsk …      ┆ …           ┆             ┆             ┆             │\n",
      "│ 3193383    ┆ Skød         ┆ En 44-årig   ┆ 44-årig     ┆ [\"Kriminali ┆ krimi       ┆ [0.108794,  │\n",
      "│            ┆ svigersøn    ┆ mormor blev  ┆ kvinde      ┆ tet\", \"Pers ┆             ┆ 0.100466, … │\n",
      "│            ┆ gennem       ┆ i dag fre…   ┆ tiltalt for ┆ onfarlig k… ┆             ┆ 0.000001]   │\n",
      "│            ┆ babydyne     ┆              ┆ drab …      ┆             ┆             ┆             │\n",
      "└────────────┴──────────────┴──────────────┴─────────────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(Dataset):\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history: pl.DataFrame\n",
    "    articles: pl.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        behaviors: pl.DataFrame,\n",
    "        history: pl.DataFrame,\n",
    "        articles: pl.DataFrame,\n",
    "        history_size: int = 30,\n",
    "        padding_value: int = 0,\n",
    "        max_length=128,\n",
    "        batch_size=32,\n",
    "        embeddings_path=None,\n",
    "    ):\n",
    "        self.behaviors = behaviors\n",
    "        self.history = history\n",
    "        self.articles = articles\n",
    "        self.history_size = history_size\n",
    "        self.padding_value = padding_value\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # TODO: I decided to instead only use pre-computed embeddings for now. You might want to look into this later down the line and implement custom embeddings (and e.g. train BERT as well).\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        # NOTE: Keep an eye on this if memory issues arise\n",
    "        self.articles = self.articles.select(\n",
    "            [\n",
    "                DEFAULT_ARTICLE_ID_COL,     # article_id\n",
    "                DEFAULT_TITLE_COL,          # title\n",
    "                DEFAULT_BODY_COL,           # body\n",
    "                DEFAULT_SUBTITLE_COL,       # subtitle\n",
    "                DEFAULT_TOPICS_COL,         # topics\n",
    "                DEFAULT_CATEGORY_STR_COL,   # category_str\n",
    "            ]\n",
    "        ).collect()\n",
    "\n",
    "        self._process_history()\n",
    "        self._prepare_training_data()\n",
    "\n",
    "    def _process_history(self):\n",
    "        self.history = (\n",
    "            self.history.select(\n",
    "                [\n",
    "                    DEFAULT_USER_COL,               # \"user_id\"\n",
    "                    DEFAULT_HISTORY_ARTICLE_ID_COL  # article_id_fixed\n",
    "                ]\n",
    "            )\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=self.history_size,\n",
    "                padding_value=self.padding_value,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "    def _prepare_training_data(self):\n",
    "        self.behaviors = self.behaviors.collect()\n",
    "\n",
    "        self.data: pl.DataFrame = (\n",
    "            slice_join_dataframes(\n",
    "                df1=self.behaviors,\n",
    "                df2=self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .select(COLUMNS)\n",
    "            .pipe(create_binary_labels_column, seed=42, label_col=DEFAULT_LABELS_COL)\n",
    "            .with_columns(\n",
    "                pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        assert self.embeddings_path is not None, \"You need to provide a path to the embeddings file.\"\n",
    "        embeddings = pl.read_parquet(self.embeddings_path)\n",
    "        print(\"Loaded Embeddings Shape:\", embeddings.shape)\n",
    "        print(\"Sample Embedding:\", embeddings.head())\n",
    "\n",
    "        self.articles = (\n",
    "            self.articles.lazy()\n",
    "            .join(embeddings.lazy(), on=DEFAULT_ARTICLE_ID_COL, how=\"inner\")\n",
    "            .rename({\"FacebookAI/xlm-roberta-base\": DEFAULT_TOKENS_COL})\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        print(\"Joined Articles with Embeddings:\", self.articles.shape)\n",
    "        print(\"Sample Article Embedding:\", self.articles.head())\n",
    "\n",
    "        article_dict = create_lookup_dict(\n",
    "            self.articles.select(DEFAULT_ARTICLE_ID_COL, DEFAULT_TOKENS_COL),\n",
    "            key=DEFAULT_ARTICLE_ID_COL,\n",
    "            value=DEFAULT_TOKENS_COL,\n",
    "        )\n",
    "\n",
    "        self.lookup_indexes, self.lookup_matrix = create_lookup_objects(\n",
    "            article_dict, unknown_representation=\"zeros\"\n",
    "        )\n",
    "\n",
    "        # print(\"Embeddings Shape:\", self.lookup_matrix.shape)\n",
    "        # print(\"Sample Embedding:\", self.lookup_matrix[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batch steps in the data\n",
    "        \"\"\"\n",
    "        return int(ceil(self.behaviors.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Get the batch of samples for the given index.\n",
    "\n",
    "        Note: The dataset class provides a single index for each iteration. The batching is done internally in this method\n",
    "        to utilize and optimize for speed. This can be seen as a mini-batching approach.\n",
    "\n",
    "        Args:\n",
    "            index (int): An integer index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the input features and labels as torch Tensors.\n",
    "                Note, the output of the PyTorch DataLoader is (1, *shape), where 1 is the DataLoader's batch_size.\n",
    "        \"\"\"\n",
    "        # Clever way to batch the data:\n",
    "        batch_indices = range(index * self.batch_size,\n",
    "                              (index + 1) * self.batch_size)\n",
    "        batch = self.data[batch_indices]\n",
    "\n",
    "        x = (\n",
    "            batch.drop(DEFAULT_LABELS_COL)\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "        )\n",
    "        # =>\n",
    "        repeats = np.array(batch[N_SAMPLES_COL])\n",
    "        # =>\n",
    "        history_input = repeat_by_list_values_from_matrix(\n",
    "            input_array=x[DEFAULT_HISTORY_ARTICLE_ID_COL].to_list(),\n",
    "            matrix=self.lookup_matrix,\n",
    "            repeats=repeats,\n",
    "        ).squeeze(2)\n",
    "        # =>\n",
    "        candidate_input = self.lookup_matrix[\n",
    "            x[DEFAULT_INVIEW_ARTICLES_COL].explode().to_list()\n",
    "        ]\n",
    "        # =>\n",
    "        history_input = torch.tensor(history_input)\n",
    "        candidate_input = torch.tensor(candidate_input)\n",
    "        y = torch.tensor(batch[DEFAULT_LABELS_COL].explode(), dtype=torch.float32).view(\n",
    "            -1, 1\n",
    "        )\n",
    "        # ========================\n",
    "        return history_input, candidate_input, y\n",
    "\n",
    "\n",
    "def load_data(tokenizer, data_path, split=\"train\", embeddings_path=None):\n",
    "    _data_path = os.path.join(data_path, split)\n",
    "\n",
    "    df_behaviors = pl.scan_parquet(_data_path + \"/behaviors.parquet\")\n",
    "    df_history = pl.scan_parquet(_data_path + \"/history.parquet\")\n",
    "    df_articles = pl.scan_parquet(data_path + \"/articles.parquet\")\n",
    "\n",
    "    return NewsDataset(tokenizer, df_behaviors, df_history, df_articles, embeddings_path=embeddings_path)\n",
    "\n",
    "\n",
    "# Model and tokenizer initialization\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "# NOTE: We need the multilingual model for the dataset\n",
    "bert = XLMRobertaModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "DATA_PATH = \"../dataset/data/ebnerd_demo\"\n",
    "EMBEDDINGS_PATH = \"../dataset/data/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet\"\n",
    "\n",
    "dataset = load_data(tokenizer, DATA_PATH, split=\"train\",\n",
    "                    embeddings_path=EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (125_541, 2)\n",
      "┌────────────┬───────────────────────────────────┐\n",
      "│ article_id ┆ FacebookAI/xlm-roberta-base       │\n",
      "│ ---        ┆ ---                               │\n",
      "│ i32        ┆ list[f32]                         │\n",
      "╞════════════╪═══════════════════════════════════╡\n",
      "│ 3000022    ┆ [0.102449, 0.101148, … -0.020715… │\n",
      "│ 3000063    ┆ [0.107297, 0.103073, … 0.004873]  │\n",
      "│ 3000613    ┆ [0.125139, 0.124621, … -0.05177]  │\n",
      "│ 3000700    ┆ [0.105697, 0.076335, … -0.034872… │\n",
      "│ 3000840    ┆ [0.098175, 0.114629, … -0.024436… │\n",
      "│ …          ┆ …                                 │\n",
      "│ 9803505    ┆ [0.11558, 0.105782, … -0.027851]  │\n",
      "│ 9803510    ┆ [0.103528, 0.124029, … -0.023985… │\n",
      "│ 9803525    ┆ [0.137623, 0.117912, … -0.018101… │\n",
      "│ 9803560    ┆ [0.14732, 0.117516, … 0.004811]   │\n",
      "│ 9803607    ┆ [0.115696, 0.116786, … -0.003788… │\n",
      "└────────────┴───────────────────────────────────┘\n",
      "shape: (5, 2)\n",
      "┌────────────┬───────────────────────────────────┐\n",
      "│ article_id ┆ FacebookAI/xlm-roberta-base       │\n",
      "│ ---        ┆ ---                               │\n",
      "│ i32        ┆ list[f32]                         │\n",
      "╞════════════╪═══════════════════════════════════╡\n",
      "│ 3000022    ┆ [0.102449, 0.101148, … -0.020715… │\n",
      "│ 3000063    ┆ [0.107297, 0.103073, … 0.004873]  │\n",
      "│ 3000613    ┆ [0.125139, 0.124621, … -0.05177]  │\n",
      "│ 3000700    ┆ [0.105697, 0.076335, … -0.034872… │\n",
      "│ 3000840    ┆ [0.098175, 0.114629, … -0.024436… │\n",
      "└────────────┴───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = \"../dataset/data/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet\"\n",
    "embeddings_df = pl.read_parquet(embeddings_path)\n",
    "\n",
    "print(embeddings_df)\n",
    "print(embeddings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[13][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids)\n",
    "        outputs = outputs[0]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Encoder\n",
    "To gain a user representation from the representations of historical clicked news, existing methods usually employ sequential (An et al., 2019) or attentive models (Wu et al., 2019d; Li et al., 2018). In this paper, we adopt additive attention as the user encoder to compress the historical information Rh. The user representation $r^u$ is then denoted as: \n",
    "\n",
    "$$ r^u = \\sum_{i=1}^I a^u_i r^h_i , a^u_i = \\text{softmax}(q^u·\\tanh(W^u r^h_i )),$$\n",
    "\n",
    " where qu and Wu are trainable parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRec(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MTRec, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, history, candidates):\n",
    "        '''\n",
    "            B - batch size (keep in mind we use an unusual mini-batch approach)\n",
    "            H - history size (number of articles in the history, usually 30)\n",
    "            D - hidden size (768)\n",
    "            history:    B x H x D \n",
    "            candidates: B x 1 x D\n",
    "        '''\n",
    "\n",
    "        # print(f\"{candidates.shape=}\")\n",
    "        att = self.q * F.tanh(self.W(history))\n",
    "        att_weight = F.softmax(att, dim=1)\n",
    "        # print(f\"{att_weight.shape=}\")\n",
    "\n",
    "        user_embedding = torch.sum(history * att_weight, dim=1)\n",
    "        # print(f\"{user_embedding.shape=}\")\n",
    "        # print(f\"{user_embedding.unsqueeze(-1).shape=}\")\n",
    "        score = torch.bmm(candidates, user_embedding.unsqueeze(-1)) # B x M x 1\n",
    "        # print(score.shape)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "    def reshape(self, batch_news, bz):\n",
    "        n_news = len(batch_news) // bz\n",
    "        reshaped_batch = batch_news.reshape(bz, n_news, -1)\n",
    "        return reshaped_batch\n",
    "    \n",
    "model = MTRec(hidden_dim=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 772:  Loss: 292.7912\r"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Assuming model and dataset are already defined\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "num_epochs = 5\n",
    "writer = SummaryWriter('runs/experiment_1')  # Specify your log directory\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with tqdm(total=num_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit='batch') as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            history, candidates, labels = batch\n",
    "            history.squeeze_(0)\n",
    "            candidates.squeeze_(0)\n",
    "            labels.squeeze_(0)\n",
    "\n",
    "            out = model(history, candidates)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Log the current loss value to TensorBoard\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * num_batches + i)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    avg_loss = running_loss / num_batches\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Log the average loss and epoch time to TensorBoard\n",
    "    writer.add_scalar('Loss/Avg_Train', avg_loss, epoch)\n",
    "    writer.add_scalar('Time/Epoch', epoch_time, epoch)\n",
    "    \n",
    "    # Display summary for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Average loss: {avg_loss:.12f}\")\n",
    "    print(f\"Epoch time: {epoch_time // 60:.0f}:{epoch_time % 60:02.0f}\\n\")\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Click Predictor\n",
    "For each candidate news, we obtain its interest score $s_j$ by matching the candidate news vector $r^c_j$ and the user representation $r^u$ via dot product: $s_j = r^c_j · r^u$. \n",
    "\n",
    "### Loss Function\n",
    "Following previous work (Huang et al., 2013; Wu et al., 2019d), we employ the NCE loss to train the main ranking model. Then the main task loss LM ain is the negative log-likelihood of all positive samples in the training dataset D: \n",
    "\n",
    "$$ \\mathcal{L}_{Main} = − \\sum^{|D|}_{i=1} \\log{\\exp(s^+_i ) \\over \\exp(s^+_i ) + \\sum^L_{j=1} \\exp(s^j_i )} $$ \n",
    "\n",
    "where $s^+$ denotes the interest scores of positive news, $L$ indicates the number of negative news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
