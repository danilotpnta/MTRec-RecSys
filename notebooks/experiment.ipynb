{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTRec:\n",
    "\n",
    "Given $I$ number historical clicked news of a user $ N^h = [n_1^h , n_2^h, ..., n^h_I ]$ and a set of $J$ candidate news $ N^c = [n^c_1, n^c_2, ..., n^c_J ] $, our goal is to calculate the user interest score $s_j$ of each candidate news according to the historical behavior of the user, then the candidate news with the highest interest score is recommended to the user. \n",
    "\n",
    "For each news, we have its title text T , category label $p^c$, and entity set E. \n",
    "\n",
    "## 2.1 News Recommendation Framework\n",
    "\n",
    "As shown in Figure 2, there are three main components in news recommendation framework, i.e., a news encoder, a user encoder, and a click predictor. \n",
    "### News Encoder\n",
    "For each news n, we encode its title with pre-trained BRET (Devlin et al., 2019). Specifically, we feed the tokenized text T into the BERT model and **adopt the embedding of [CLS] token as the news representation r**. \n",
    "\n",
    "We denote the encoded vectors of historical clicked news $N^h$ and candidate news $N^c$ as $R^h = [r^h_1 , r^h_2 , ..., r^h_I ]$ and $R^c = [r^c_1, r^c_2, ..., r^c_J ]$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ebrec.utils._constants import DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_IMPRESSION_ID_COL, DEFAULT_ARTICLE_ID_COL, DEFAULT_TITLE_COL, DEFAULT_BODY_COL\n",
    "from ebrec.utils._behaviors import truncate_history, create_binary_labels_column\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaModel, RobertaTokenizer, XLMRobertaModel, XLMRobertaTokenizer\n",
    "\n",
    "# NOTE: We need the multilingual model for the dataset\n",
    "model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "bert = XLMRobertaModel.from_pretrained(model_name)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_BODY_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_LABELS_COL\n",
    ")\n",
    "DEFAULT_TOKENS_COL = \"tokens\"\n",
    "N_SAMPLES_COL = \"n_samples\"\n",
    "\n",
    "from ebrec.utils._python import (\n",
    "    generate_unique_name,\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    "    create_lookup_dict,\n",
    ")\n",
    "\n",
    "# Honestly, massive respect to that guy for writing all of these utilities but I REALLY REALLY prefer not trying to scour them from 10 different files. Better code quality would be to have it all inside the function. DRY is overrated.\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def map_list_article_id_to_value(\n",
    "    behaviors: pl.DataFrame,\n",
    "    behaviors_column: str,\n",
    "    mapping: dict[int, pl.Series],\n",
    "    drop_nulls: bool = False,\n",
    "    fill_nulls: any = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Maps the values of a column in a DataFrame `behaviors` containing article IDs to their corresponding values\n",
    "    in a column in another DataFrame `articles`. The mapping is performed using a dictionary constructed from\n",
    "    the two DataFrames. The resulting DataFrame has the same columns as `behaviors`, but with the article IDs\n",
    "    replaced by their corresponding values.\n",
    "\n",
    "    Args:\n",
    "        behaviors (pl.DataFrame): The DataFrame containing the column to be mapped.\n",
    "        behaviors_column (str): The name of the column to be mapped in `behaviors`.\n",
    "        mapping (dict[int, pl.Series]): A dictionary with article IDs as keys and corresponding values as values.\n",
    "            Note, 'replace' works a lot faster when values are of type pl.Series!\n",
    "        drop_nulls (bool): If `True`, any rows in the resulting DataFrame with null values will be dropped.\n",
    "            If `False` and `fill_nulls` is specified, null values in `behaviors_column` will be replaced with `fill_null`.\n",
    "        fill_nulls (Optional[any]): If specified, any null values in `behaviors_column` will be replaced with this value.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the same columns as `behaviors`, but with the article IDs in\n",
    "            `behaviors_column` replaced by their corresponding values in `mapping`.\n",
    "\n",
    "    Example:\n",
    "    >>> behaviors = pl.DataFrame(\n",
    "            {\"user_id\": [1, 2, 3, 4, 5], \"article_ids\": [[\"A1\", \"A2\"], [\"A2\", \"A3\"], [\"A1\", \"A4\"], [\"A4\", \"A4\"], None]}\n",
    "        )\n",
    "    >>> articles = pl.DataFrame(\n",
    "            {\n",
    "                \"article_id\": [\"A1\", \"A2\", \"A3\"],\n",
    "                \"article_type\": [\"News\", \"Sports\", \"Entertainment\"],\n",
    "            }\n",
    "        )\n",
    "    >>> articles_dict = dict(zip(articles[\"article_id\"], articles[\"article_type\"]))\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            fill_nulls=\"Unknown\",\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", \"Unknown\"]         │\n",
    "        │ 4       ┆ [\"Unknown\", \"Unknown\"]      │\n",
    "        │ 5       ┆ [\"Unknown\"]                 │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=True,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\"]                    │\n",
    "        │ 4       ┆ null                        │\n",
    "        │ 5       ┆ null                        │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", null]              │\n",
    "        │ 4       ┆ [null, null]                │\n",
    "        │ 5       ┆ [null]                      │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    \"\"\"\n",
    "    GROUPBY_ID = generate_unique_name(behaviors.columns, \"_groupby_id\")\n",
    "    behaviors = behaviors.lazy().with_row_index(GROUPBY_ID)\n",
    "    # =>\n",
    "    select_column = (\n",
    "        behaviors.select(pl.col(GROUPBY_ID), pl.col(behaviors_column))\n",
    "        .explode(behaviors_column)\n",
    "        .with_columns(pl.col(behaviors_column).replace(mapping, default=None))\n",
    "        .collect()\n",
    "    )\n",
    "    # =>\n",
    "    if drop_nulls:\n",
    "        select_column = select_column.drop_nulls()\n",
    "    elif fill_nulls is not None:\n",
    "        select_column = select_column.with_columns(\n",
    "            pl.col(behaviors_column).fill_null(fill_nulls)\n",
    "        )\n",
    "    # =>\n",
    "    select_column = (\n",
    "        select_column.lazy().group_by(GROUPBY_ID).agg(behaviors_column).collect()\n",
    "    )\n",
    "    return (\n",
    "        behaviors.drop(behaviors_column)\n",
    "        .collect()\n",
    "        .join(select_column, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "    )\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    behaviors: pl.DataFrame\n",
    "    history: pl.DataFrame\n",
    "    articles: pl.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        behaviors: pl.DataFrame,\n",
    "        history: pl.DataFrame,\n",
    "        articles: pl.DataFrame,\n",
    "        history_size: int = 30,\n",
    "        padding_value: int = 0,\n",
    "        max_length=128,\n",
    "        batch_size=32,\n",
    "        embeddings_path = None,\n",
    "    ):\n",
    "        self.behaviors = behaviors\n",
    "        self.history = history\n",
    "        self.articles = articles\n",
    "\n",
    "        self.history_size = history_size\n",
    "        self.padding_value = padding_value\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.max_length = max_length\n",
    "        \n",
    "        # TODO: I decided to instead only use pre-computed embeddings for now. You might want to look into this later down the line and implement custom embeddings (and e.g. train BERT as well).\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        # NOTE: Keep an eye on this if memory issues arise\n",
    "        self.articles = self.articles.select(\n",
    "            [\n",
    "                DEFAULT_ARTICLE_ID_COL,\n",
    "                DEFAULT_TITLE_COL,\n",
    "                DEFAULT_BODY_COL,\n",
    "                DEFAULT_SUBTITLE_COL,\n",
    "                DEFAULT_TOPICS_COL,\n",
    "                DEFAULT_CATEGORY_STR_COL,\n",
    "            ]\n",
    "        ).collect()\n",
    "\n",
    "        self._process_history()\n",
    "        self._set_data()\n",
    "\n",
    "    def _process_history(self):\n",
    "        self.history = (\n",
    "            self.history.select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=self.history_size,\n",
    "                padding_value=self.padding_value,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "    def _set_data(self):\n",
    "        self.behaviors = self.behaviors.collect()\n",
    "        self.data: pl.DataFrame = (\n",
    "            slice_join_dataframes(\n",
    "                self.behaviors,\n",
    "                self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .select(COLUMNS)\n",
    "            .pipe(create_binary_labels_column, seed=42, label_col=DEFAULT_LABELS_COL)\n",
    "        ).with_columns(\n",
    "            pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL)\n",
    "        )\n",
    "\n",
    "        assert self.embeddings_path is not None, \"You need to provide a path to the embeddings file.\"\n",
    "\n",
    "        embeddings = pl.scan_parquet(self.embeddings_path)\n",
    "\n",
    "        self.articles = self.articles.lazy().join(embeddings, on=DEFAULT_ARTICLE_ID_COL, how=\"inner\").rename({\"FacebookAI/xlm-roberta-base\": DEFAULT_TOKENS_COL}).collect()\n",
    "\n",
    "        article_dict = create_lookup_dict(\n",
    "            self.articles.select(DEFAULT_ARTICLE_ID_COL, DEFAULT_TOKENS_COL),\n",
    "            key=DEFAULT_ARTICLE_ID_COL,\n",
    "            value=DEFAULT_TOKENS_COL,\n",
    "        )\n",
    "\n",
    "        self.lookup_indexes, self.lookup_matrix = create_lookup_objects(\n",
    "            article_dict, unknown_representation=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batch steps in the data\n",
    "        \"\"\"\n",
    "        return int(ceil(self.behaviors.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Get the batch of samples for the given index.\n",
    "\n",
    "        Note: The dataset class provides a single index for each iteration. The batching is done internally in this method\n",
    "        to utilize and optimize for speed. This can be seen as a mini-batching approach.\n",
    "\n",
    "        Args:\n",
    "            index (int): An integer index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the input features and labels as torch Tensors.\n",
    "                Note, the output of the PyTorch DataLoader is (1, *shape), where 1 is the DataLoader's batch_size.\n",
    "        \"\"\"\n",
    "        # Clever way to batch the data:\n",
    "        batch_indices = range(index * self.batch_size, (index + 1) * self.batch_size)\n",
    "        batch = self.data[batch_indices]\n",
    "\n",
    "        x = (\n",
    "            batch.drop(DEFAULT_LABELS_COL)\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "        )\n",
    "        # =>\n",
    "        repeats = np.array(batch[N_SAMPLES_COL])\n",
    "        # =>\n",
    "        history_input = repeat_by_list_values_from_matrix(\n",
    "            input_array=x[DEFAULT_HISTORY_ARTICLE_ID_COL].to_list(),\n",
    "            matrix=self.lookup_matrix,\n",
    "            repeats=repeats,\n",
    "        ).squeeze(2)\n",
    "        # =>\n",
    "        candidate_input = self.lookup_matrix[\n",
    "            x[DEFAULT_INVIEW_ARTICLES_COL].explode().to_list()\n",
    "        ]\n",
    "        # =>\n",
    "        history_input = torch.tensor(history_input)\n",
    "        candidate_input = torch.tensor(candidate_input)\n",
    "        y = torch.tensor(batch[DEFAULT_LABELS_COL].explode(), dtype=torch.float32).view(\n",
    "            -1, 1\n",
    "        )\n",
    "        # ========================\n",
    "        return history_input, candidate_input, y\n",
    "\n",
    "\n",
    "import os.path\n",
    "\n",
    "\n",
    "def load_data(tokenizer, data_path, split=\"train\", embeddings_path=None):\n",
    "    _data_path = os.path.join(data_path, split)\n",
    "\n",
    "    df_behaviors = pl.scan_parquet(_data_path + \"/behaviors.parquet\")\n",
    "    df_history = pl.scan_parquet(_data_path + \"/history.parquet\")\n",
    "    df_articles = pl.scan_parquet(data_path + \"/articles.parquet\")\n",
    "\n",
    "    return NewsDataset(tokenizer, df_behaviors, df_history, df_articles, embeddings_path=embeddings_path)\n",
    "\n",
    "\n",
    "data_path = \"../dataset/data/ebnerd_demo\"\n",
    "dataset = load_data(tokenizer, data_path, split=\"train\", embeddings_path=\"../dataset/data/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[13][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids)\n",
    "        outputs = outputs.last_hidden_state[:, 0, :]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Encoder\n",
    "To gain a user representation from the representations of historical clicked news, existing methods usually employ sequential (An et al., 2019) or attentive models (Wu et al., 2019d; Li et al., 2018). In this paper, we adopt additive attention as the user encoder to compress the historical information Rh. The user representation $r^u$ is then denoted as: \n",
    "\n",
    "$$ r^u = \\sum_{i=1}^I a^u_i r^h_i , a^u_i = \\text{softmax}(q^u·\\tanh(W^u r^h_i )),$$\n",
    "\n",
    " where qu and Wu are trainable parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRec(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MTRec, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, history, candidates):\n",
    "        '''\n",
    "            B - batch size (keep in mind we use an unusual mini-batch approach)\n",
    "            H - history size (number of articles in the history, usually 30)\n",
    "            D - hidden size (768)\n",
    "            history:    B x H x D \n",
    "            candidates: B x 1 x D\n",
    "        '''\n",
    "\n",
    "        # print(f\"{candidates.shape=}\")\n",
    "        att = self.q * F.tanh(self.W(history))\n",
    "        att_weight = F.softmax(att, dim=1)\n",
    "        # print(f\"{att_weight.shape=}\")\n",
    "\n",
    "        user_embedding = torch.sum(history * att_weight, dim = 1)\n",
    "        # print(f\"{user_embedding.shape=}\")\n",
    "        # print(f\"{user_embedding.unsqueeze(-1).shape=}\")\n",
    "        score = torch.bmm(candidates, user_embedding.unsqueeze(-1)) # B x M x 1\n",
    "        # print(score.shape)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "    def reshape(self, batch_news, bz):\n",
    "        n_news = len(batch_news) // bz\n",
    "        reshaped_batch = batch_news.reshape(bz, n_news, -1)\n",
    "        return reshaped_batch\n",
    "    \n",
    "model = MTRec(hidden_dim=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:41,  4.79batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:24,  5.35batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:21,  5.45batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:26,  5.25batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:21,  5.46batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:21,  5.44batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:30,  5.13batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:26,  5.26batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:26,  5.28batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:05,  4.16batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:55,  4.40batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:34,  5.00batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:03,  4.21batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:56,  4.37batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<04:09,  3.10batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<04:40,  2.75batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:48,  3.38batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:21,  3.84batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:21,  3.83batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:33,  3.61batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:07,  4.11batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:44,  4.68batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:58,  4.32batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:28,  5.22batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:30,  5.12batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:55,  4.39batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:59,  4.29batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:21,  3.84batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:03,  4.20batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<03:09,  4.07batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:27,  5.22batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:36,  4.94batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:32,  5.06batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:32,  5.06batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:21,  5.45batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:27,  5.24batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:23,  5.39batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:24,  5.36batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:26,  5.28batch/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/773 [00:00<02:25,  5.30batch/s, loss=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "\n",
    "def positive_nll(y_pred, y_true):\n",
    "    \"\"\"The following function computes the negative log likelihood for the positive class.\n",
    "    \n",
    "    $$ \\mathcal{L} = − \\sum^{|D|}_{i=1} \\log{\\exp(s^+_i ) \\over \\exp(s^+_i ) + \\sum^L_{j=1} \\exp(s^j_i )} $$ \n",
    "    \"\"\"\n",
    "    pos = y_true * y_pred.exp()\n",
    "    neg = torch.sum((~y_true) * y_pred.exp(), dim=0)\n",
    "    print(pos, neg)\n",
    "    return -torch.log(pos.sum(1) / (pos.sum(1) + neg + 1e-6)).sum()\n",
    "    \n",
    "\n",
    "epochs = 40\n",
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    losses = []\n",
    "    with tqdm(total=len(dataloader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            history, candidates, labels = batch\n",
    "            history.squeeze_(0)\n",
    "            candidates.squeeze_(0)\n",
    "            labels.squeeze_(0)\n",
    "            labels = labels.to(torch.bool)\n",
    "\n",
    "            out = model(history, candidates)\n",
    "            # out[labels==0] = 0\n",
    "\n",
    "            # loss = nn.Gaussian(out, labels)\n",
    "            # loss = positive_nll(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            pos = labels * out.exp()\n",
    "            neg = torch.sum((~labels) * out.exp(), dim=0)\n",
    "            # print(pos, neg)\n",
    "            loss = -torch.log(pos.sum(1) / (pos.sum(1) + neg + 1e-6)).sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({\"loss\": round(losses[-1], 3)})\n",
    "            pbar.update(1)\n",
    "            break\n",
    "    print(\"Average loss:\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(history, candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Click Predictor\n",
    "For each candidate news, we obtain its interest score $s_j$ by matching the candidate news vector $r^c_j$ and the user representation $r^u$ via dot product: $s_j = r^c_j · r^u$. \n",
    "\n",
    "### Loss Function\n",
    "Following previous work (Huang et al., 2013; Wu et al., 2019d), we employ the NCE loss to train the main ranking model. Then the main task loss LM ain is the negative log-likelihood of all positive samples in the training dataset D: \n",
    "\n",
    "$$ \\mathcal{L}_{Main} = − \\sum^{|D|}_{i=1} \\log{\\exp(s^+_i ) \\over \\exp(s^+_i ) + \\sum^L_{j=1} \\exp(s^j_i )} $$ \n",
    "\n",
    "where $s^+$ denotes the interest scores of positive news, $L$ indicates the number of negative news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
