{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTRec:\n",
    "\n",
    "Given $I$ number historical clicked news of a user $ N^h = [n_1^h , n_2^h, ..., n^h_I ]$ and a set of $J$ candidate news $ N^c = [n^c_1, n^c_2, ..., n^c_J ] $, our goal is to calculate the user interest score $s_j$ of each candidate news according to the historical behavior of the user, then the candidate news with the highest interest score is recommended to the user. \n",
    "\n",
    "For each news, we have its title text T , category label $p^c$, and entity set E. \n",
    "\n",
    "## 2.1 News Recommendation Framework\n",
    "\n",
    "As shown in Figure 2, there are three main components in news recommendation framework, i.e., a news encoder, a user encoder, and a click predictor. \n",
    "### News Encoder\n",
    "For each news n, we encode its title with pre-trained BRET (Devlin et al., 2019). Specifically, we feed the tokenized text T into the BERT model and **adopt the embedding of [CLS] token as the news representation r**. \n",
    "\n",
    "We denote the encoded vectors of historical clicked news $N^h$ and candidate news $N^c$ as $R^h = [r^h_1 , r^h_2 , ..., r^h_I ]$ and $R^c = [r^c_1, r^c_2, ..., r^c_J ]$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation & Maths imports\n",
    "import os.path\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from pprint import pprint\n",
    "\n",
    "# Torch & Transformer imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer, \n",
    "    RobertaModel, \n",
    "    RobertaTokenizer, \n",
    "    XLMRobertaModel, \n",
    "    XLMRobertaTokenizer\n",
    ")\n",
    "\n",
    "# ebrec constants\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,         # article_id\n",
    "    DEFAULT_TITLE_COL,              # title\n",
    "    DEFAULT_BODY_COL,               # body\n",
    "    DEFAULT_SUBTITLE_COL,           # subtitle\n",
    "    DEFAULT_TOPICS_COL,             # topics\n",
    "    DEFAULT_CATEGORY_STR_COL,       # category_str\n",
    "    DEFAULT_LABELS_COL,             # labels\n",
    "    DEFAULT_USER_COL,               # user_id\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL, # article_id_fixed\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,    # article_ids_inview\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,   # article_ids_clicked\n",
    "    DEFAULT_IMPRESSION_ID_COL       # impression_id\n",
    ")\n",
    "\n",
    "# ebrec utils\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._behaviors import truncate_history, create_binary_labels_column\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "from ebrec.utils._python import (\n",
    "    generate_unique_name,\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    "    create_lookup_dict,\n",
    ")\n",
    "\n",
    "# Columns to be used in the dataset processing\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,                # \"user_id\"\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,  # \"article_id_fixed\"\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,     # \"article_ids_inview\"\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,    # \"article_ids_clicked\"\n",
    "    DEFAULT_IMPRESSION_ID_COL,       # \"impression_id\"\n",
    "]\n",
    "DEFAULT_TOKENS_COL = \"tokens\"\n",
    "N_SAMPLES_COL = \"n_samples\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "class PolarsDataFrameWrapper:\n",
    "    \"\"\"DataFrame wrapper for Polars DataFrame to enable slicing with step and make it interoperable with torch.Dataset objects.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            # Handle negative indexing\n",
    "            if key < 0:\n",
    "                key += self.data.height\n",
    "            if key < 0 or key >= self.data.height:\n",
    "                raise IndexError(\"Index out of bounds\")\n",
    "            # Fetch a single row as a DataFrame\n",
    "            return self.data.slice(key, 1)\n",
    "        elif isinstance(key, slice):\n",
    "            start, stop, step = key.start, key.stop, key.step\n",
    "            # Adjust for negative indexing and None values\n",
    "            if start is None:\n",
    "                start = 0\n",
    "            elif start < 0:\n",
    "                start += self.data.height\n",
    "            if stop is None:\n",
    "                stop = self.data.height\n",
    "            elif stop < 0:\n",
    "                stop += self.data.height\n",
    "            # Calculate length for slice\n",
    "            length = stop - start\n",
    "            if step is None or step == 1:\n",
    "                return self.data.slice(start, length)\n",
    "            else:\n",
    "                # For steps other than 1, use take with a list of indices\n",
    "                indices = range(start, stop, step)\n",
    "                return self.data[indices]\n",
    "        else:\n",
    "            raise TypeError(\"Invalid argument type.\")\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history: pl.DataFrame\n",
    "    articles: pl.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        behaviors: pl.DataFrame,\n",
    "        history: pl.DataFrame,\n",
    "        articles: pl.DataFrame,\n",
    "        history_size: int = 30,\n",
    "        padding_value: int = 0,\n",
    "        max_length=128,\n",
    "        batch_size=32,\n",
    "        embeddings_path=None,\n",
    "    ):\n",
    "        self.behaviors = behaviors\n",
    "        self.history = history\n",
    "        self.articles = articles\n",
    "        self.history_size = history_size\n",
    "        self.padding_value = padding_value\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # TODO: I decided to instead only use pre-computed embeddings for now. You might want to look into this later down the line and implement custom embeddings (and e.g. train BERT as well).\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        # NOTE: Keep an eye on this if memory issues arise\n",
    "        self.articles = self.articles.select(\n",
    "            [\n",
    "                DEFAULT_ARTICLE_ID_COL,     # article_id\n",
    "                DEFAULT_TITLE_COL,          # title\n",
    "                DEFAULT_BODY_COL,           # body\n",
    "                DEFAULT_SUBTITLE_COL,       # subtitle\n",
    "                DEFAULT_TOPICS_COL,         # topics\n",
    "                DEFAULT_CATEGORY_STR_COL,   # category_str\n",
    "            ]\n",
    "        ).collect()\n",
    "\n",
    "        self._process_history()\n",
    "        self._prepare_training_data()\n",
    "\n",
    "    def _process_history(self):\n",
    "        self.history = (\n",
    "            self.history.select(\n",
    "                [\n",
    "                    DEFAULT_USER_COL,               # \"user_id\"\n",
    "                    DEFAULT_HISTORY_ARTICLE_ID_COL  # article_id_fixed\n",
    "                ]\n",
    "            )\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=self.history_size,\n",
    "                padding_value=self.padding_value,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "    def _prepare_training_data(self):\n",
    "        self.behaviors = self.behaviors.collect()\n",
    "\n",
    "        def sort_and_select(df: pl.DataFrame, n: int = 5, labels_col: str = DEFAULT_LABELS_COL, inview_col: str = DEFAULT_INVIEW_ARTICLES_COL):\n",
    "            \"\"\"Selects the first clicked article and n-1 random articles from the inview articles.\"\"\"\n",
    "            a, b = [], []\n",
    "            for i, x in enumerate(df[labels_col]):\n",
    "                idx = np.argsort(x)\n",
    "                idx = np.concatenate((idx[:n-1],idx[-1:]))\n",
    "                shuffle(idx)\n",
    "                a.append(x[idx])\n",
    "                b.append(df[inview_col][i][idx])\n",
    "\n",
    "            return df.with_columns(pl.Series(a).alias(labels_col), pl.Series(b).alias(inview_col))\n",
    "\n",
    "        self.data: pl.DataFrame = (\n",
    "            slice_join_dataframes(\n",
    "                df1=self.behaviors,\n",
    "                df2=self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .select(COLUMNS)\n",
    "            .pipe(create_binary_labels_column, seed=42, label_col=DEFAULT_LABELS_COL)\n",
    "            .pipe(sort_and_select, n=5)\n",
    "            .with_columns(\n",
    "                pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.data = PolarsDataFrameWrapper(self.data)\n",
    "\n",
    "        assert self.embeddings_path is not None, \"You need to provide a path to the embeddings file.\"\n",
    "        embeddings = pl.read_parquet(self.embeddings_path)\n",
    "\n",
    "        self.articles = (\n",
    "            self.articles.lazy()\n",
    "            .join(embeddings.lazy(), on=DEFAULT_ARTICLE_ID_COL, how=\"inner\")\n",
    "            .rename({\"FacebookAI/xlm-roberta-base\": DEFAULT_TOKENS_COL})\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        article_dict = create_lookup_dict(\n",
    "            self.articles.select(DEFAULT_ARTICLE_ID_COL, DEFAULT_TOKENS_COL),\n",
    "            key=DEFAULT_ARTICLE_ID_COL,\n",
    "            value=DEFAULT_TOKENS_COL,\n",
    "        )\n",
    "\n",
    "        self.lookup_indexes, self.lookup_matrix = create_lookup_objects(\n",
    "            article_dict, unknown_representation=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batch steps in the data\n",
    "        \"\"\"\n",
    "        return int(ceil(self.behaviors.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get the samples for the given index.\n",
    "        \n",
    "        Args:\n",
    "            index (int): An integer or a slice index.\n",
    "\n",
    "        Returns:\n",
    "            history: torch.Tensor: The history input features.\n",
    "            candidate: torch.Tensor: The candidate input features.\n",
    "            y: torch.Tensor: The target labels.\n",
    "        \"\"\"\n",
    "\n",
    "        batch = self.data[index]\n",
    "        # ========================\n",
    "        x = (\n",
    "            batch.drop(DEFAULT_LABELS_COL)\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "        )\n",
    "        # =>\n",
    "        history_input = self.lookup_matrix[\n",
    "            x[DEFAULT_HISTORY_ARTICLE_ID_COL].to_list()\n",
    "        ]\n",
    "        # =>\n",
    "        candidate_input = self.lookup_matrix[\n",
    "            x[DEFAULT_INVIEW_ARTICLES_COL].to_list()\n",
    "        ]\n",
    "        # =>\n",
    "        history_input = torch.tensor(history_input).squeeze()\n",
    "        candidate_input = torch.tensor(candidate_input).squeeze()\n",
    "        y = torch.tensor(batch[DEFAULT_LABELS_COL]).squeeze()\n",
    "        # ========================\n",
    "        return history_input, candidate_input, y\n",
    "\n",
    "\n",
    "def load_data(tokenizer, data_path, split=\"train\", embeddings_path=None):\n",
    "    _data_path = os.path.join(data_path, split)\n",
    "\n",
    "    df_behaviors = pl.scan_parquet(_data_path + \"/behaviors.parquet\")\n",
    "    df_history = pl.scan_parquet(_data_path + \"/history.parquet\")\n",
    "    df_articles = pl.scan_parquet(data_path + \"/articles.parquet\")\n",
    "\n",
    "    return NewsDataset(tokenizer, df_behaviors, df_history, df_articles, embeddings_path=embeddings_path)\n",
    "\n",
    "\n",
    "# Model and tokenizer initialization\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "# NOTE: We need the multilingual model for the dataset\n",
    "# bert = XLMRobertaModel.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "DATA_PATH = \"../data/demo\"\n",
    "EMBEDDINGS_PATH = \"../data/FacebookAI-xlm-roberta-base/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet\"\n",
    "\n",
    "tokenizer = None\n",
    "dataset = load_data(tokenizer, DATA_PATH, split=\"train\",\n",
    "                    embeddings_path=EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30, 768])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(dataset, batch_size=32, shuffle=True)))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort_and_select(df: pl.DataFrame, n: int = 5, labels_col: str = DEFAULT_LABELS_COL, inview_col: str = DEFAULT_INVIEW_ARTICLES_COL):\n",
    "    \"\"\"Selects the first clicked article and n-1 random articles from the inview articles.\"\"\"\n",
    "    a, b = [], []\n",
    "    for i, x in enumerate(df[labels_col]):\n",
    "        idx = np.argsort(x)\n",
    "        idx = np.concatenate((idx[:n-1],idx[-1:]))\n",
    "        shuffle(idx)\n",
    "        a.append(x[idx])\n",
    "        b.append(dataset.data[inview_col][i][idx])\n",
    "\n",
    "    return df.with_columns(pl.Series(a).alias(labels_col), pl.Series(b).alias(inview_col))\n",
    "\n",
    "sort_and_select(dataset.data)\n",
    "# dataset.data[DEFAULT_LABELS_COL].list.take([[0,1,2,3,4]]*10).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDatasetV2(NewsDataset):\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = dataset.data[index]\n",
    "        sample = sample.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            mapping=dataset.lookup_indexes,\n",
    "            fill_nulls=[0],\n",
    "\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "            mapping=dataset.lookup_indexes,\n",
    "            fill_nulls=[0],\n",
    "        )\n",
    "\n",
    "        _history = sample[DEFAULT_HISTORY_ARTICLE_ID_COL].explode().explode().to_list()\n",
    "        history = torch.from_numpy(dataset.lookup_matrix[_history])\n",
    "        _candidates = sample[DEFAULT_INVIEW_ARTICLES_COL].explode().explode().to_list()\n",
    "        candidates = torch.from_numpy(dataset.lookup_matrix[_candidates])\n",
    "        # dataset.lookup_indexes\n",
    "        labels = torch.tensor(sample[DEFAULT_LABELS_COL].to_list()[0])\n",
    "        return history, candidates, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 768\n",
    "W = nn.Linear(hidden_dim, hidden_dim)\n",
    "q = nn.Parameter(torch.randn(hidden_dim))\n",
    "# dataset = NewsDatasetV2(tokenizer, dataset.behaviors.lazy(), dataset.history.lazy(), dataset.articles.lazy(), embeddings_path=EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338, 30, 768]) torch.Size([338, 1, 768]) torch.Size([338, 1])\n"
     ]
    }
   ],
   "source": [
    "history, candidates, labels = dataset[0]\n",
    "print(history.shape, candidates.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys.model import MultitaskRecommender\n",
    "\n",
    "model = MultitaskRecommender(hidden_dim=hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "model.validation_step((history, candidates, labels, None), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8447689 , 0.79280245, 0.8391594 , 0.8859233 , 0.7581733 ,\n",
       "       0.8147102 , 0.7981994 , 0.7677725 , 0.87957144, 0.78188485,\n",
       "       0.85107934, 0.86420804, 0.8369788 , 0.78540623, 0.7139053 ,\n",
       "       0.87063205, 0.83668715, 0.8741352 , 0.7119443 , 0.76597345,\n",
       "       0.8069536 , 0.8142832 , 0.9063672 , 0.8481541 , 0.8841946 ,\n",
       "       0.83461213, 0.9158705 , 0.90282834, 0.7485722 , 0.80206317,\n",
       "       0.88614917, 0.9070107 , 0.81787723, 0.83281165, 0.7046031 ,\n",
       "       0.87564373, 0.81582254, 0.8240167 , 0.87787825, 0.6839086 ,\n",
       "       0.87530255, 0.8541871 , 0.90109986, 0.8816683 , 0.9160575 ,\n",
       "       0.77996516, 0.7400617 , 0.8159844 , 0.8222676 , 0.8732511 ,\n",
       "       0.7234934 , 0.8263015 , 0.7153633 , 0.6891647 , 0.86385506,\n",
       "       0.88955647, 0.8002839 , 0.8695692 , 0.7599167 , 0.8768085 ,\n",
       "       0.852402  , 0.92176795, 0.6982013 , 0.87994224, 0.8225002 ,\n",
       "       0.7594265 , 0.8190207 , 0.82410777, 0.8226401 , 0.7999426 ,\n",
       "       0.71022576, 0.89975107, 0.7757452 , 0.711023  , 0.717053  ,\n",
       "       0.8502597 , 0.85284466, 0.6907939 , 0.8516798 , 0.7196861 ,\n",
       "       0.7359847 , 0.74272335, 0.8796594 , 0.88499546, 0.8610428 ,\n",
       "       0.86761665, 0.88908833, 0.6977358 , 0.85655856, 0.72797245,\n",
       "       0.7859966 , 0.7014625 , 0.8662557 , 0.82098365, 0.82551926,\n",
       "       0.86642116, 0.90490365, 0.8397242 , 0.72354656, 0.8896085 ,\n",
       "       0.8794832 , 0.8774605 , 0.88296664, 0.7151184 , 0.8830758 ,\n",
       "       0.87878615, 0.81020015, 0.8828649 , 0.83938694, 0.8294353 ,\n",
       "       0.9083734 , 0.90445125, 0.69069046, 0.76225144, 0.84064674,\n",
       "       0.8269181 , 0.8313347 , 0.78829896, 0.7694038 , 0.83894235,\n",
       "       0.89669895, 0.8691525 , 0.86837655, 0.7013874 , 0.8140337 ,\n",
       "       0.84334934, 0.84338796, 0.90741324, 0.8702588 , 0.7299966 ,\n",
       "       0.814277  , 0.8596849 , 0.8738654 , 0.7031183 , 0.90137124,\n",
       "       0.7940139 , 0.82844484, 0.8367453 , 0.804988  , 0.832731  ,\n",
       "       0.8301321 , 0.81873184, 0.8255802 , 0.7813303 , 0.8307263 ,\n",
       "       0.8853169 , 0.7432055 , 0.87427527, 0.8162896 , 0.9075493 ,\n",
       "       0.83142775, 0.7743466 , 0.82083535, 0.7216094 , 0.81235653,\n",
       "       0.6993549 , 0.6599259 , 0.660547  , 0.64661694, 0.56491566,\n",
       "       0.6267494 , 0.6350425 , 0.83195275, 0.84132665, 0.8872115 ,\n",
       "       0.85932124, 0.8323415 , 0.8607656 , 0.808774  , 0.8656801 ,\n",
       "       0.90166974, 0.8550686 , 0.8241631 , 0.84640706, 0.8110408 ,\n",
       "       0.89700216, 0.8736271 , 0.80648446, 0.63835424, 0.74666977,\n",
       "       0.8057924 , 0.78788626, 0.84460443, 0.87204146, 0.8670692 ,\n",
       "       0.9150704 , 0.8226296 , 0.81773615, 0.7969909 , 0.8780969 ,\n",
       "       0.8158062 , 0.7401273 , 0.85906875, 0.7960964 , 0.8489502 ,\n",
       "       0.8792028 , 0.9024211 , 0.8792832 , 0.8802977 , 0.8779768 ,\n",
       "       0.78287727, 0.8597299 , 0.8588753 , 0.6388265 , 0.8793421 ,\n",
       "       0.81532925, 0.87122905, 0.8514265 , 0.82398206, 0.8968737 ,\n",
       "       0.9063274 , 0.79906815, 0.91731256, 0.8459794 , 0.88110125,\n",
       "       0.83446324, 0.6957671 , 0.88870394, 0.7970415 , 0.8006249 ,\n",
       "       0.7276258 , 0.7716554 , 0.79936355, 0.8360589 , 0.87984973,\n",
       "       0.8508992 , 0.79607815, 0.8928043 , 0.857491  , 0.8454363 ,\n",
       "       0.8085922 , 0.692544  , 0.82274425, 0.83967745, 0.8069294 ,\n",
       "       0.87557924, 0.8041032 , 0.7440881 , 0.8554989 , 0.8920892 ,\n",
       "       0.850615  , 0.7576595 , 0.74576   , 0.8343839 , 0.72097254,\n",
       "       0.729195  , 0.812789  , 0.87955767, 0.82505614, 0.7982857 ,\n",
       "       0.8307681 , 0.88352644, 0.8465337 , 0.8714017 , 0.8961889 ,\n",
       "       0.87952316, 0.86718315, 0.89189595, 0.8536628 , 0.8019909 ,\n",
       "       0.80173546, 0.7676494 , 0.8051307 , 0.79825294, 0.8266893 ,\n",
       "       0.83840716, 0.8208938 , 0.8681312 , 0.8897429 , 0.8616137 ,\n",
       "       0.8605473 , 0.6996975 , 0.79888463, 0.8594803 , 0.826829  ,\n",
       "       0.8781304 , 0.8392151 , 0.8382384 , 0.8330945 , 0.8186502 ,\n",
       "       0.76145995, 0.712154  , 0.79204714, 0.77759004, 0.76377535,\n",
       "       0.78225523, 0.83101076, 0.84461516, 0.85100895, 0.7759289 ,\n",
       "       0.87265563, 0.77045757, 0.8078994 , 0.80984664, 0.82003564,\n",
       "       0.87976843, 0.80007434, 0.7947233 , 0.7485869 , 0.808577  ,\n",
       "       0.88375217, 0.8739745 , 0.8511933 , 0.8847199 , 0.77195966,\n",
       "       0.86753905, 0.8711461 , 0.75602525, 0.86700034, 0.89236426,\n",
       "       0.8177683 , 0.84642   , 0.82932645, 0.8529099 , 0.7897094 ,\n",
       "       0.8655921 , 0.8566526 , 0.82536227, 0.79270494, 0.83871907,\n",
       "       0.83629054, 0.7500966 , 0.87160337, 0.76914984, 0.7317817 ,\n",
       "       0.8750265 , 0.84634185, 0.78213876, 0.7824756 , 0.8310561 ,\n",
       "       0.7333305 , 0.8480772 , 0.77896994, 0.7947797 , 0.8729673 ,\n",
       "       0.7012534 , 0.83030015, 0.85875386], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predictions[0] = torch.tensor(model.predictions[0]).sigmoid().numpy()\n",
    "model.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5,\n",
       "    \"mrr\": 0.019565572598611122,\n",
       "    \"ndcg@10\": 0.1100458831490401,\n",
       "    \"logloss\": 22.93047750798735,\n",
       "    \"rmse\": 0.9514859318733215,\n",
       "    \"accuracy\": 0.09467455621301775,\n",
       "    \"f1\": 0.17297297297297298\n",
       "}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metric_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = q * F.tanh(W(history))\n",
    "att_weight = F.softmax(att, dim=-1)\n",
    "\n",
    "user_embedding = torch.sum(history * att_weight, dim=0)\n",
    "\n",
    "# print(f\"{user_embedding.shape=}\")\n",
    "# print(f\"{user_embedding.unsqueeze(-1).shape=}\")\n",
    "# score = torch.bmm(candidates, user_embedding.unsqueeze(-1)) # B x M x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.9078, 10.8969, 10.8815, 10.8896, 10.8903, 10.8746, 10.9196, 10.8738,\n",
       "        10.8796, 10.8819, 10.8866], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.bmm(candidates.unsqueeze(0), user_embedding.unsqueeze(-1).unsqueeze(0)).squeeze()\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_nll_loss(preds, labels):\n",
    "    pos = preds[labels == 1].exp()\n",
    "    neg = preds[labels == 0].exp().sum(-1)\n",
    "    return - torch.log(pos / (pos + neg)).mean()\n",
    "\n",
    "# positive_nll_loss(scores.squeeze(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24724 [00:00<?, ?it/s, loss=2.41]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 675/24724 [00:14<08:28, 47.32it/s, loss=1.94]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jh/2h78bdyj1qzfbls2j1yfbv9c0000gn/T/ipykernel_22362/364244854.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0matt_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jh/2h78bdyj1qzfbls2j1yfbv9c0000gn/T/ipykernel_22362/3890473861.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0m_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_HISTORY_ARTICLE_ID_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_INVIEW_ARTICLES_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_candidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# dataset.lookup_indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_LABELS_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/polars/series/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_accessor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnamespace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/polars/expr/expr.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m-> 4444\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4445\u001b[0m         \"\"\"\n\u001b[1;32m   4446\u001b[0m         \u001b[0mExplode\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mexpression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam([q, W.weight, W.bias], lr=0.0005)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    losses = []\n",
    "    q.requires = True\n",
    "    W.training = True\n",
    "    with tqdm(dataset) as pbar:\n",
    "        for batch in pbar:\n",
    "            history, candidates, labels = batch\n",
    "            att = q * F.tanh(W(history))\n",
    "            att_weight = F.softmax(att, dim=-1)\n",
    "            user_embedding = torch.sum(history * att_weight, dim=0)\n",
    "            scores = torch.bmm(candidates.unsqueeze(0), user_embedding.unsqueeze(-1).unsqueeze(0)).squeeze()\n",
    "            loss = positive_nll_loss(scores.squeeze(), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=losses[-1])\n",
    "    print(f\"Average Loss: {np.mean(losses)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids)\n",
    "        outputs = outputs.last_hidden_state[:, 0, :]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Encoder\n",
    "To gain a user representation from the representations of historical clicked news, existing methods usually employ sequential (An et al., 2019) or attentive models (Wu et al., 2019d; Li et al., 2018). In this paper, we adopt additive attention as the user encoder to compress the historical information Rh. The user representation $r^u$ is then denoted as: \n",
    "\n",
    "$$ \\textbf{r}^u = \\sum_{i=1}^I \\textbf{a}^u_i \\textbf{r}^h_i , \\textbf{a}^u_i = \\text{softmax}(\\textbf{q}^u·\\tanh(W^u r^h_i )),$$\n",
    "\n",
    " where qu and Wu are trainable parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRec(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MTRec, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, history, candidates):\n",
    "        '''\n",
    "            B - batch size (keep in mind we use an unusual mini-batch approach)\n",
    "            H - history size (number of articles in the history, usually 30)\n",
    "            D - hidden size (768)\n",
    "            history:    B x H x D \n",
    "            candidates: B x 1 x D\n",
    "        '''\n",
    "\n",
    "        # print(f\"{candidates.shape=}\")\n",
    "        att = self.q * F.tanh(self.W(history))\n",
    "        att_weight = F.softmax(att, dim=1)\n",
    "        # print(f\"{att_weight.shape=}\")\n",
    "\n",
    "        user_embedding = torch.sum(history * att_weight, dim=1)\n",
    "        # print(f\"{user_embedding.shape=}\")\n",
    "        # print(f\"{user_embedding.unsqueeze(-1).shape=}\")\n",
    "        score = torch.bmm(candidates, user_embedding.unsqueeze(-1)) # B x M x 1\n",
    "        # print(score.shape)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "    def reshape(self, batch_news, bz):\n",
    "        n_news = len(batch_news) // bz\n",
    "        reshaped_batch = batch_news.reshape(bz, n_news, -1)\n",
    "        return reshaped_batch\n",
    "    \n",
    "model = MTRec(hidden_dim=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 772:  Loss: 292.7912\r"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Assuming model and dataset are already defined\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "num_epochs = 5\n",
    "writer = SummaryWriter('runs/experiment_1')  # Specify your log directory\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    losses = []\n",
    "    with tqdm(total=num_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit='batch') as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            history, candidates, labels = batch\n",
    "            history.squeeze_(0)\n",
    "            candidates.squeeze_(0)\n",
    "            labels.squeeze_(0)\n",
    "            labels = labels.to(torch.bool)\n",
    "\n",
    "            out = model(history, candidates)\n",
    "            # out[labels==0] = 0\n",
    "\n",
    "            # loss = nn.Gaussian(out, labels)\n",
    "            # loss = positive_nll(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            pos = labels * out.exp()\n",
    "            neg = torch.sum((~labels) * out.exp(), dim=0)\n",
    "            # print(pos, neg)\n",
    "            loss = -torch.log(pos.sum(1) / (pos.sum(1) + neg + 1e-6)).sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({\"loss\": round(losses[-1], 3)})\n",
    "            pbar.update(1)\n",
    "            break\n",
    "    print(\"Average loss:\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#             out = model(history, candidates)\n",
    "#             loss = criterion(out, labels)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "            \n",
    "#             # Log the current loss value to TensorBoard\n",
    "#             writer.add_scalar('Loss/Train', loss.item(), epoch * num_batches + i)\n",
    "            \n",
    "#             # Update the progress bar\n",
    "#             pbar.set_postfix(loss=loss.item())\n",
    "#             pbar.update(1)\n",
    "    \n",
    "#     avg_loss = running_loss / num_batches\n",
    "#     epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "#     # Log the average loss and epoch time to TensorBoard\n",
    "#     writer.add_scalar('Loss/Avg_Train', avg_loss, epoch)\n",
    "#     writer.add_scalar('Time/Epoch', epoch_time, epoch)\n",
    "    \n",
    "#     # Display summary for the epoch\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Average loss: {avg_loss:.12f}\")\n",
    "#     print(f\"Epoch time: {epoch_time // 60:.0f}:{epoch_time % 60:02.0f}\\n\")\n",
    "\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Click Predictor\n",
    "For each candidate news, we obtain its interest score $s_j$ by matching the candidate news vector $r^c_j$ and the user representation $r^u$ via dot product: $s_j = r^c_j · r^u$. \n",
    "\n",
    "### Loss Function\n",
    "Following previous work (Huang et al., 2013; Wu et al., 2019d), we employ the NCE loss to train the main ranking model. Then the main task loss LM ain is the negative log-likelihood of all positive samples in the training dataset D: \n",
    "\n",
    "$$ \\mathcal{L}_{Main} = − \\sum^{|D|}_{i=1} \\log{\\exp(s^+_i ) \\over \\exp(s^+_i ) + \\sum^L_{j=1} \\exp(s^j_i )} $$ \n",
    "\n",
    "where $s^+$ denotes the interest scores of positive news, $L$ indicates the number of negative news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
