{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTRec:\n",
    "\n",
    "Given $I$ number historical clicked news of a user $ N^h = [n_1^h , n_2^h, ..., n^h_I ]$ and a set of $J$ candidate news $ N^c = [n^c_1, n^c_2, ..., n^c_J ] $, our goal is to calculate the user interest score $s_j$ of each candidate news according to the historical behavior of the user, then the candidate news with the highest interest score is recommended to the user. \n",
    "\n",
    "For each news, we have its title text T , category label $p^c$, and entity set E. \n",
    "\n",
    "## 2.1 News Recommendation Framework\n",
    "\n",
    "As shown in Figure 2, there are three main components in news recommendation framework, i.e., a news encoder, a user encoder, and a click predictor. \n",
    "### News Encoder\n",
    "For each news n, we encode its title with pre-trained BRET (Devlin et al., 2019). Specifically, we feed the tokenized text T into the BERT model and **adopt the embedding of [CLS] token as the news representation r**. \n",
    "\n",
    "We denote the encoded vectors of historical clicked news $N^h$ and candidate news $N^c$ as $R^h = [r^h_1 , r^h_2 , ..., r^h_I ]$ and $R^c = [r^c_1, r^c_2, ..., r^c_J ]$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ebrec.utils._constants import DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_IMPRESSION_ID_COL, DEFAULT_ARTICLE_ID_COL, DEFAULT_TITLE_COL, DEFAULT_BODY_COL\n",
    "from ebrec.utils._behaviors import truncate_history, create_binary_labels_column\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaModel, RobertaTokenizer, XLMRobertaModel, XLMRobertaTokenizer\n",
    "\n",
    "# NOTE: We need the multilingual model for the dataset\n",
    "model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "bert = XLMRobertaModel.from_pretrained(model_name)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_BODY_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_LABELS_COL\n",
    ")\n",
    "DEFAULT_TOKENS_COL = \"tokens\"\n",
    "N_SAMPLES_COL = \"n_samples\"\n",
    "\n",
    "from ebrec.utils._python import (\n",
    "    generate_unique_name,\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    "    create_lookup_dict,\n",
    ")\n",
    "\n",
    "# Honestly, massive respect to that guy for writing all of these utilities but I REALLY REALLY prefer not trying to scour them from 10 different files. Better code quality would be to have it all inside the function. DRY is overrated.\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def map_list_article_id_to_value(\n",
    "    behaviors: pl.DataFrame,\n",
    "    behaviors_column: str,\n",
    "    mapping: dict[int, pl.Series],\n",
    "    drop_nulls: bool = False,\n",
    "    fill_nulls: any = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Maps the values of a column in a DataFrame `behaviors` containing article IDs to their corresponding values\n",
    "    in a column in another DataFrame `articles`. The mapping is performed using a dictionary constructed from\n",
    "    the two DataFrames. The resulting DataFrame has the same columns as `behaviors`, but with the article IDs\n",
    "    replaced by their corresponding values.\n",
    "\n",
    "    Args:\n",
    "        behaviors (pl.DataFrame): The DataFrame containing the column to be mapped.\n",
    "        behaviors_column (str): The name of the column to be mapped in `behaviors`.\n",
    "        mapping (dict[int, pl.Series]): A dictionary with article IDs as keys and corresponding values as values.\n",
    "            Note, 'replace' works a lot faster when values are of type pl.Series!\n",
    "        drop_nulls (bool): If `True`, any rows in the resulting DataFrame with null values will be dropped.\n",
    "            If `False` and `fill_nulls` is specified, null values in `behaviors_column` will be replaced with `fill_null`.\n",
    "        fill_nulls (Optional[any]): If specified, any null values in `behaviors_column` will be replaced with this value.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the same columns as `behaviors`, but with the article IDs in\n",
    "            `behaviors_column` replaced by their corresponding values in `mapping`.\n",
    "\n",
    "    Example:\n",
    "    >>> behaviors = pl.DataFrame(\n",
    "            {\"user_id\": [1, 2, 3, 4, 5], \"article_ids\": [[\"A1\", \"A2\"], [\"A2\", \"A3\"], [\"A1\", \"A4\"], [\"A4\", \"A4\"], None]}\n",
    "        )\n",
    "    >>> articles = pl.DataFrame(\n",
    "            {\n",
    "                \"article_id\": [\"A1\", \"A2\", \"A3\"],\n",
    "                \"article_type\": [\"News\", \"Sports\", \"Entertainment\"],\n",
    "            }\n",
    "        )\n",
    "    >>> articles_dict = dict(zip(articles[\"article_id\"], articles[\"article_type\"]))\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            fill_nulls=\"Unknown\",\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", \"Unknown\"]         │\n",
    "        │ 4       ┆ [\"Unknown\", \"Unknown\"]      │\n",
    "        │ 5       ┆ [\"Unknown\"]                 │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=True,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\"]                    │\n",
    "        │ 4       ┆ null                        │\n",
    "        │ 5       ┆ null                        │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", null]              │\n",
    "        │ 4       ┆ [null, null]                │\n",
    "        │ 5       ┆ [null]                      │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    \"\"\"\n",
    "    GROUPBY_ID = generate_unique_name(behaviors.columns, \"_groupby_id\")\n",
    "    behaviors = behaviors.lazy().with_row_index(GROUPBY_ID)\n",
    "    # =>\n",
    "    select_column = (\n",
    "        behaviors.select(pl.col(GROUPBY_ID), pl.col(behaviors_column))\n",
    "        .explode(behaviors_column)\n",
    "        .with_columns(pl.col(behaviors_column).replace(mapping, default=None))\n",
    "        .collect()\n",
    "    )\n",
    "    # =>\n",
    "    if drop_nulls:\n",
    "        select_column = select_column.drop_nulls()\n",
    "    elif fill_nulls is not None:\n",
    "        select_column = select_column.with_columns(\n",
    "            pl.col(behaviors_column).fill_null(fill_nulls)\n",
    "        )\n",
    "    # =>\n",
    "    select_column = (\n",
    "        select_column.lazy().group_by(GROUPBY_ID).agg(behaviors_column).collect()\n",
    "    )\n",
    "    return (\n",
    "        behaviors.drop(behaviors_column)\n",
    "        .collect()\n",
    "        .join(select_column, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "    )\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    behaviors: pl.DataFrame\n",
    "    history: pl.DataFrame\n",
    "    articles: pl.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        behaviors: pl.DataFrame,\n",
    "        history: pl.DataFrame,\n",
    "        articles: pl.DataFrame,\n",
    "        history_size: int = 30,\n",
    "        padding_value: int = 0,\n",
    "        max_length=128,\n",
    "        batch_size=32,\n",
    "        embeddings_path = None,\n",
    "    ):\n",
    "        self.behaviors = behaviors\n",
    "        self.history = history\n",
    "        self.articles = articles\n",
    "\n",
    "        self.history_size = history_size\n",
    "        self.padding_value = padding_value\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.max_length = max_length\n",
    "        \n",
    "        # TODO: I decided to instead only use pre-computed embeddings for now. You might want to look into this later down the line and implement custom embeddings (and e.g. train BERT as well).\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        # NOTE: Keep an eye on this if memory issues arise\n",
    "        self.articles = self.articles.select(\n",
    "            [\n",
    "                DEFAULT_ARTICLE_ID_COL,\n",
    "                DEFAULT_TITLE_COL,\n",
    "                DEFAULT_BODY_COL,\n",
    "                DEFAULT_SUBTITLE_COL,\n",
    "                DEFAULT_TOPICS_COL,\n",
    "                DEFAULT_CATEGORY_STR_COL,\n",
    "            ]\n",
    "        ).collect()\n",
    "\n",
    "        self._process_history()\n",
    "        self._set_data()\n",
    "\n",
    "    def _process_history(self):\n",
    "        self.history = (\n",
    "            self.history.select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=self.history_size,\n",
    "                padding_value=self.padding_value,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "    def _set_data(self):\n",
    "        self.behaviors = self.behaviors.collect()\n",
    "        self.data: pl.DataFrame = (\n",
    "            slice_join_dataframes(\n",
    "                self.behaviors,\n",
    "                self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .select(COLUMNS)\n",
    "            .pipe(create_binary_labels_column, seed=42, label_col=DEFAULT_LABELS_COL)\n",
    "        ).with_columns(\n",
    "            pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL)\n",
    "        )\n",
    "\n",
    "        assert self.embeddings_path is not None, \"You need to provide a path to the embeddings file.\"\n",
    "\n",
    "        embeddings = pl.scan_parquet(self.embeddings_path)\n",
    "\n",
    "        self.articles = self.articles.lazy().join(embeddings, on=DEFAULT_ARTICLE_ID_COL, how=\"inner\").rename({\"FacebookAI/xlm-roberta-base\": DEFAULT_TOKENS_COL}).collect()\n",
    "\n",
    "        article_dict = create_lookup_dict(\n",
    "            self.articles.select(DEFAULT_ARTICLE_ID_COL, DEFAULT_TOKENS_COL),\n",
    "            key=DEFAULT_ARTICLE_ID_COL,\n",
    "            value=DEFAULT_TOKENS_COL,\n",
    "        )\n",
    "\n",
    "        self.lookup_indexes, self.lookup_matrix = create_lookup_objects(\n",
    "            article_dict, unknown_representation=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batch steps in the data\n",
    "        \"\"\"\n",
    "        return int(ceil(self.behaviors.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Get the batch of samples for the given index.\n",
    "\n",
    "        Note: The dataset class provides a single index for each iteration. The batching is done internally in this method\n",
    "        to utilize and optimize for speed. This can be seen as a mini-batching approach.\n",
    "\n",
    "        Args:\n",
    "            index (int): An integer index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the input features and labels as torch Tensors.\n",
    "                Note, the output of the PyTorch DataLoader is (1, *shape), where 1 is the DataLoader's batch_size.\n",
    "        \"\"\"\n",
    "        # Clever way to batch the data:\n",
    "        batch_indices = range(index * self.batch_size, (index + 1) * self.batch_size)\n",
    "        batch = self.data[batch_indices]\n",
    "\n",
    "        x = (\n",
    "            batch.drop(DEFAULT_LABELS_COL)\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "                mapping=self.lookup_indexes,\n",
    "                fill_nulls=[0],\n",
    "            )\n",
    "        )\n",
    "        # =>\n",
    "        repeats = np.array(batch[N_SAMPLES_COL])\n",
    "        # =>\n",
    "        history_input = repeat_by_list_values_from_matrix(\n",
    "            input_array=x[DEFAULT_HISTORY_ARTICLE_ID_COL].to_list(),\n",
    "            matrix=self.lookup_matrix,\n",
    "            repeats=repeats,\n",
    "        ).squeeze(2)\n",
    "        # =>\n",
    "        candidate_input = self.lookup_matrix[\n",
    "            x[DEFAULT_INVIEW_ARTICLES_COL].explode().to_list()\n",
    "        ]\n",
    "        # =>\n",
    "        history_input = torch.tensor(history_input)\n",
    "        candidate_input = torch.tensor(candidate_input)\n",
    "        y = torch.tensor(batch[DEFAULT_LABELS_COL].explode(), dtype=torch.float32).view(\n",
    "            -1, 1\n",
    "        )\n",
    "        # ========================\n",
    "        return history_input, candidate_input, y\n",
    "\n",
    "\n",
    "import os.path\n",
    "\n",
    "\n",
    "def load_data(tokenizer, data_path, split=\"train\", embeddings_path=None):\n",
    "    _data_path = os.path.join(data_path, split)\n",
    "\n",
    "    df_behaviors = pl.scan_parquet(_data_path + \"/behaviors.parquet\")\n",
    "    df_history = pl.scan_parquet(_data_path + \"/history.parquet\")\n",
    "    df_articles = pl.scan_parquet(data_path + \"/articles.parquet\")\n",
    "\n",
    "    return NewsDataset(tokenizer, df_behaviors, df_history, df_articles, embeddings_path=embeddings_path)\n",
    "\n",
    "\n",
    "data_path = \"../dataset/data/ebnerd_demo\"\n",
    "dataset = load_data(tokenizer, data_path, split=\"train\", embeddings_path=\"../dataset/data/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[13][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids)\n",
    "        outputs = outputs[0]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Encoder\n",
    "To gain a user representation from the representations of historical clicked news, existing methods usually employ sequential (An et al., 2019) or attentive models (Wu et al., 2019d; Li et al., 2018). In this paper, we adopt additive attention as the user encoder to compress the historical information Rh. The user representation $r^u$ is then denoted as: \n",
    "\n",
    "$$ r^u = \\sum_{i=1}^I a^u_i r^h_i , a^u_i = \\text{softmax}(q^u·\\tanh(W^u r^h_i )),$$\n",
    "\n",
    " where qu and Wu are trainable parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRec(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MTRec, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, history, candidates):\n",
    "        '''\n",
    "            B - batch size (keep in mind we use an unusual mini-batch approach)\n",
    "            H - history size (number of articles in the history, usually 30)\n",
    "            D - hidden size (768)\n",
    "            history:    B x H x D \n",
    "            candidates: B x 1 x D\n",
    "        '''\n",
    "\n",
    "        # print(f\"{candidates.shape=}\")\n",
    "        att = self.q * F.tanh(self.W(history))\n",
    "        att_weight = F.softmax(att, dim=1)\n",
    "        # print(f\"{att_weight.shape=}\")\n",
    "\n",
    "        user_embedding = torch.sum(history * att_weight, dim = 1)\n",
    "        # print(f\"{user_embedding.shape=}\")\n",
    "        # print(f\"{user_embedding.unsqueeze(-1).shape=}\")\n",
    "        score = torch.bmm(candidates, user_embedding.unsqueeze(-1)) # B x M x 1\n",
    "        # print(score.shape)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "    def reshape(self, batch_news, bz):\n",
    "        n_news = len(batch_news) // bz\n",
    "        reshaped_batch = batch_news.reshape(bz, n_news, -1)\n",
    "        return reshaped_batch\n",
    "    \n",
    "model = MTRec(hidden_dim=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 164:  Loss: -0.0000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(    )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    \n",
    "\n",
    "    history, candidates, labels = batch\n",
    "    # print(f\"{history.shape=}\")\n",
    "    # print(f\"{candidates.shape=}\")\n",
    "    # print(f\"{labels.shape=}\")\n",
    "    \n",
    "    history.squeeze_(0)\n",
    "    candidates.squeeze_(0)\n",
    "    labels.squeeze_(0)\n",
    "\n",
    "    out = model(history, candidates)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    optimizer.zero_grad(    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step: {i:>3d}:  Loss: {loss.item():.4f}\", end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Click Predictor\n",
    "For each candidate news, we obtain its interest score $s_j$ by matching the candidate news vector $r^c_j$ and the user representation $r^u$ via dot product: $s_j = r^c_j · r^u$. \n",
    "\n",
    "### Loss Function\n",
    "Following previous work (Huang et al., 2013; Wu et al., 2019d), we employ the NCE loss to train the main ranking model. Then the main task loss LM ain is the negative log-likelihood of all positive samples in the training dataset D: \n",
    "\n",
    "$$ \\mathcal{L}_{Main} = − \\sum^{|D|}_{i=1} \\log{\\exp(s^+_i ) \\over \\exp(s^+_i ) + \\sum^L_{j=1} \\exp(s^j_i )} $$ \n",
    "\n",
    "where $s^+$ denotes the interest scores of positive news, $L$ indicates the number of negative news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
