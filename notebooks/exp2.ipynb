{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from recsys.model import MultitaskRecommender\n",
    "from recsys.dataset import NewsDataModule\n",
    "\n",
    "if \"datamodule\" not in locals():\n",
    "    datamodule = NewsDataModule(\"../data\", batch_size=32)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()\n",
    "\n",
    "model = MultitaskRecommender(768, n_categories=datamodule.train_dataset.max_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3037230</td><td>&quot;Ishockey-spill…</td><td>&quot;ISHOCKEY: Isho…</td><td>2023-06-29 06:20:57</td><td>false</td><td>&quot;Ambitionerne o…</td><td>2003-08-28 08:55:00</td><td>null</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Mindre ulykke&quot;]</td><td>142</td><td>[327, 334]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr><tr><td>3044020</td><td>&quot;Prins Harry tv…</td><td>&quot;Hoffet tvang P…</td><td>2023-06-29 06:21:16</td><td>false</td><td>&quot;Den britiske t…</td><td>2005-06-29 08:47:00</td><td>[3097307, 3097197, 3104927]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[&quot;Harry&quot;, &quot;James Hewitt&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Personfarlig kriminalitet&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7084</td><td>&quot;Negative&quot;</td></tr><tr><td>3057622</td><td>&quot;Rådden kørsel …</td><td>&quot;Kan ikke straf…</td><td>2023-06-29 06:21:24</td><td>false</td><td>&quot;Slingrende spr…</td><td>2005-10-10 07:20:00</td><td>[3047102]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Transportmiddel&quot;, &quot;Bil&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9236</td><td>&quot;Negative&quot;</td></tr><tr><td>3073151</td><td>&quot;Mærsk-arvinger…</td><td>&quot;FANGET I FLODB…</td><td>2023-06-29 06:21:38</td><td>false</td><td>&quot;To oldebørn af…</td><td>2005-01-04 06:59:00</td><td>[3067474, 3067478, 3153705]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Privat virksomhed&quot;, … &quot;Rejse&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9945</td><td>&quot;Negative&quot;</td></tr><tr><td>3193383</td><td>&quot;Skød svigersøn…</td><td>&quot;44-årig kvinde…</td><td>2023-06-29 06:22:57</td><td>false</td><td>&quot;En 44-årig mor…</td><td>2003-09-15 15:30:00</td><td>null</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9966</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3037230   ┆ Ishockey- ┆ ISHOCKEY: ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ spiller:  ┆ Ishockey- ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Jeg       ┆ spilleren ┆ 06:20:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ troede    ┆ Seb…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ jeg…      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3044020   ┆ Prins     ┆ Hoffet    ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7084    ┆ Negative │\n",
       "│           ┆ Harry     ┆ tvang     ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tvunget   ┆ Prins     ┆ 06:21:16  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ til       ┆ Harry til ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ dna-test  ┆ at …      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3057622   ┆ Rådden    ┆ Kan ikke  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9236    ┆ Negative │\n",
       "│           ┆ kørsel på ┆ straffes: ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ blå       ┆ Udenlands ┆ 06:21:24  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ plader    ┆ ke d…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3073151   ┆ Mærsk-arv ┆ FANGET I  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9945    ┆ Negative │\n",
       "│           ┆ inger i   ┆ FLODBØLGE ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ livsfare  ┆ N: Skibsr ┆ 06:21:38  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ edere…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3193383   ┆ Skød      ┆ 44-årig   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9966    ┆ Negative │\n",
       "│           ┆ svigersøn ┆ kvinde    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gennem    ┆ tiltalt   ┆ 06:22:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ babydyne  ┆ for drab  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "articles = pl.read_parquet(\"../data/demo/articles.parquet\")\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from typing import Any, Literal\n",
    "from recsys.dataset import NewsDataset, batch_random_choice_with_reset, sort_and_select\n",
    "from recsys.utils.classes import PolarsDataFrameWrapper\n",
    "from recsys.utils.download import CHALLENGE_DATASET, download_file, unzip_file\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from ebrec.utils._behaviors import create_binary_labels_column, truncate_history\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_BODY_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_CATEGORY_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "from ebrec.utils._python import (\n",
    "    create_lookup_dict,\n",
    "    create_lookup_objects,\n",
    "    generate_unique_name,\n",
    ")\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "import json\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "\n",
    "DEFAULT_TOKENS_COL = \"tokens\"\n",
    "N_SAMPLES_COL = \"n_samples\"\n",
    "HISTORY_TITLES_COL = \"history_titles\"\n",
    "INVIEW_TITLES_COL = \"inview_titles\"\n",
    "\n",
    "\n",
    "class NewsDatasetV2(TorchDataset):\n",
    "    behaviors: pl.DataFrame\n",
    "    history: pl.DataFrame\n",
    "    articles: pl.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        behaviors: pl.DataFrame,\n",
    "        history: pl.DataFrame,\n",
    "        articles: pl.DataFrame,\n",
    "        history_size: int = 30,\n",
    "        max_labels: int = 5,\n",
    "        padding_value: int = 0,\n",
    "        max_length=128,\n",
    "        test_mode=False,\n",
    "    ):\n",
    "        self.behaviors = behaviors\n",
    "        self.history = history\n",
    "        self.articles = articles\n",
    "        self.history_size = history_size\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_labels = max_labels\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        # NOTE: Keep an eye on this if memory issues arise\n",
    "        self.articles = self.articles.select(\n",
    "            [\n",
    "                DEFAULT_ARTICLE_ID_COL,  # article_id\n",
    "                DEFAULT_TITLE_COL,  # title\n",
    "                DEFAULT_BODY_COL,  # body\n",
    "                DEFAULT_SUBTITLE_COL,  # subtitle\n",
    "                DEFAULT_TOPICS_COL,  # topics\n",
    "                DEFAULT_CATEGORY_STR_COL,  # category_str\n",
    "            ]\n",
    "        ).collect()\n",
    "\n",
    "        self.history = self._process_history(self.history, history_size, padding_value)\n",
    "        # Prepare the actual training data\n",
    "        self.behaviors = self.behaviors.collect()\n",
    "        self._prepare_articles()\n",
    "\n",
    "        if test_mode:\n",
    "            self._prepare_test_data()\n",
    "        else:\n",
    "            self._prepare_training_data()\n",
    "\n",
    "    def save_preprocessed(self, path: str):\n",
    "        \"\"\"Save the preprocessed data to the given path directory.\"\"\"\n",
    "        data = {\n",
    "            \"history_size\": self.history_size,\n",
    "            \"padding_value\": self.padding_value,\n",
    "            \"max_labels\": self.max_labels,\n",
    "            \"max_categories\": self.max_categories,\n",
    "            \"test_mode\": self.test_mode,\n",
    "        }\n",
    "\n",
    "        with open(path + \"/parameters.json\", \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "        self.lookup_matrix.save_to_disk(path + \"/lookup_matrix\")\n",
    "        self.behaviors.write_parquet(path + \"/behaviors.parquet\")\n",
    "        self.history.write_parquet(path + \"/history.parquet\")\n",
    "        self.articles.write_parquet(path + \"/articles.parquet\")\n",
    "        self.data.dataframe.write_parquet(path + \"/data.parquet\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_preprocessed(path: str):\n",
    "        \"\"\"Load the preprocessed data from the given path directory.\"\"\"\n",
    "        dataset = NewsDataset.__new__(NewsDataset)\n",
    "        with open(path + \"/parameters.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset.history_size = data[\"history_size\"]\n",
    "            dataset.padding_value = data[\"padding_value\"]\n",
    "            dataset.max_labels = data[\"max_labels\"]\n",
    "            dataset.max_categories = data[\"max_categories\"]\n",
    "            dataset.test_mode = data[\"test_mode\"]\n",
    "\n",
    "        dataset.lookup_matrix = Dataset.load_from_disk(path + \"/lookup_matrix\")\n",
    "        dataset.behaviors = pl.read_parquet(path + \"/behaviors.parquet\")\n",
    "        dataset.history = pl.read_parquet(path + \"/history.parquet\")\n",
    "        dataset.articles = pl.read_parquet(path + \"/articles.parquet\")\n",
    "        dataset.data = PolarsDataFrameWrapper(pl.read_parquet(path + \"/data.parquet\"))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @classmethod\n",
    "    def _process_history(\n",
    "        cls, history: pl.LazyFrame, history_size: int = 30, padding_value: int = 0\n",
    "    ) -> pl.DataFrame:\n",
    "        return (\n",
    "            history.select(\n",
    "                [\n",
    "                    DEFAULT_USER_COL,  # user_id\n",
    "                    DEFAULT_HISTORY_ARTICLE_ID_COL,  # article_id_fixed\n",
    "                ]\n",
    "            )\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=history_size,\n",
    "                padding_value=padding_value,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "\n",
    "    def _prepare_articles(self):\n",
    "        self.articles = (\n",
    "            self.articles.lazy()\n",
    "            .with_columns(\n",
    "                pl.col(DEFAULT_CATEGORY_STR_COL)\n",
    "                .cast(pl.Categorical)\n",
    "                .to_physical()\n",
    "                .alias(DEFAULT_CATEGORY_COL)\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer([\"\"] + self.articles[DEFAULT_TITLE_COL].to_list(), truncation=True, padding=True)\n",
    "        \n",
    "        # Create the lookup matrix        \n",
    "        self.lookup_matrix = Dataset.from_dict(tokens).add_column(DEFAULT_CATEGORY_COL, [0] + self.articles[DEFAULT_CATEGORY_COL].cast(pl.UInt8).to_list())\n",
    "    \n",
    "        self.max_categories = max(self.lookup_matrix[DEFAULT_CATEGORY_COL]) + 1\n",
    "        self.article_id_to_idx = {k: i for i, k in enumerate([0] + self.articles[DEFAULT_ARTICLE_ID_COL].to_list())}\n",
    "\n",
    "    def _prepare_test_data(self):\n",
    "        self.data = (\n",
    "            slice_join_dataframes(\n",
    "                df1=self.behaviors,\n",
    "                df2=self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            ).select(COLUMNS[:-1])  # do not count clicked articles as these do not exist in test\n",
    "        )\n",
    "        \n",
    "        self.data = self.data.with_columns(\n",
    "            pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.eval(pl.element().replace(self.article_id_to_idx, default=0)),\n",
    "            pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.eval(pl.element().replace(self.article_id_to_idx, default=0))\n",
    "        )\n",
    "\n",
    "        self.data = PolarsDataFrameWrapper(self.data)\n",
    "\n",
    "\n",
    "\n",
    "    def _prepare_training_data(self):\n",
    "\n",
    "        # Map article_id to index\n",
    "\n",
    "        self.data = (\n",
    "            slice_join_dataframes(\n",
    "                df1=self.behaviors,\n",
    "                df2=self.history,\n",
    "                on=DEFAULT_USER_COL,\n",
    "                how=\"left\",\n",
    "            ).select(COLUMNS)\n",
    "            .pipe(create_binary_labels_column, label_col=DEFAULT_LABELS_COL, shuffle=False)\n",
    "            .pipe(sort_and_select, n=self.max_labels)\n",
    "            .with_columns(pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL))\n",
    "        )\n",
    "        \n",
    "        self.data = self.data.with_columns(\n",
    "            pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.eval(pl.element().replace(self.article_id_to_idx, default=0)),\n",
    "            pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.eval(pl.element().replace(self.article_id_to_idx, default=0))\n",
    "        )\n",
    "        \n",
    "        self.data = PolarsDataFrameWrapper(self.data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get the samples for the given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): An integer or a slice index.\n",
    "\n",
    "        Returns:\n",
    "            history: torch.Tensor: The history input features.\n",
    "            candidate: torch.Tensor: The candidate input features.\n",
    "            y: torch.Tensor: The target labels.\n",
    "        \"\"\"\n",
    "\n",
    "        batch = self.data[index]\n",
    "        \n",
    "        # Construct the history vectors\n",
    "        _hist = list(self.lookup_matrix[__hist] for __hist in batch[DEFAULT_HISTORY_ARTICLE_ID_COL].to_list())\n",
    "        histories = {key: torch.tensor([val[key] for val in _hist]) for key in self.lookup_matrix.features.keys()}\n",
    "        \n",
    "\n",
    "        # Early return for test mode\n",
    "        # ========================\n",
    "        # Construct the candidate vectors\n",
    "        _cand = list(self.lookup_matrix[__cand] for __cand in batch[DEFAULT_INVIEW_ARTICLES_COL].to_list())\n",
    "        if self.test_mode:\n",
    "            # Special treatment, as they are not guaranteed to be of the same length\n",
    "            candidates = {key: [torch.tensor(val[key]) for val in _cand] for key in self.lookup_matrix.features.keys()}\n",
    "            return histories, candidates\n",
    "        # ========================\n",
    "\n",
    "        labels = batch[DEFAULT_LABELS_COL].to_list()\n",
    "        candidates = {key: torch.tensor([val[key] for val in _cand]) for key in self.lookup_matrix.features.keys()}\n",
    "        y = torch.tensor(labels).float().squeeze()\n",
    "        # # ========================\n",
    "        return histories, candidates, y\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "# tokens = tokenizer([\"\"] + articles[\"title\"].to_list(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun\n",
    "from recsys.dataset import load_data\n",
    "\n",
    "behaviors, history, articles = load_data(\"../data/small\", \"train\")\n",
    "dataset = NewsDatasetV2(tokenizer, behaviors, history, articles, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<recsys.dataset.NewsDataset at 0x1803c6e90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewsDatasetV2.from_preprocessed(\"../data/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'behaviors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     slice_join_dataframes(\n\u001b[0;32m----> 3\u001b[0m         df1\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbehaviors\u001b[49m,\n\u001b[1;32m      4\u001b[0m         df2\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m      5\u001b[0m         on\u001b[38;5;241m=\u001b[39mDEFAULT_USER_COL,\n\u001b[1;32m      6\u001b[0m         how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     )\u001b[38;5;241m.\u001b[39mselect(COLUMNS)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(DEFAULT_INVIEW_ARTICLES_COL)\u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39meval(pl\u001b[38;5;241m.\u001b[39melement()\u001b[38;5;241m.\u001b[39meq(DEFAULT_CLICKED_ARTICLES_COL))\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mUInt8))\u001b[38;5;241m.\u001b[39malias(DEFAULT_LABELS_COL)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(DEFAULT_LABELS_COL)\u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39mlen()\u001b[38;5;241m.\u001b[39malias(N_SAMPLES_COL))\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'behaviors'"
     ]
    }
   ],
   "source": [
    "dataset.data = (\n",
    "    slice_join_dataframes(\n",
    "        df1=dataset.behaviors,\n",
    "        df2=dataset.history,\n",
    "        on=DEFAULT_USER_COL,\n",
    "        how=\"left\",\n",
    "    ).select(COLUMNS)\n",
    "    .with_columns(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.eval(pl.element().eq(DEFAULT_CLICKED_ARTICLES_COL)).cast(pl.UInt8)).alias(DEFAULT_LABELS_COL)\n",
    "    .with_columns(pl.col(DEFAULT_LABELS_COL).list.len().alias(N_SAMPLES_COL))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7424)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.predictions[0]\n",
    "labels = model.labels[0]\n",
    "from torch.nn import functional as F\n",
    "\n",
    "F.binary_cross_entropy_with_logits(model.predictions[0], model.labels[0].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (batch_size, candidates)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (batch_size, candidates)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/project/MTRec-RecSys/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "print(scores.shape) # (batch_size, candidates)\n",
    "print(labels.shape) # (batch_size, candidates)\n",
    "\n",
    "# loss = F.cross_entropy(scores, labels)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels[0].T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
